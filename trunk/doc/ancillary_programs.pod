=head1 Tesserae Ancillary Programs

=head2 Overview

This document describes how to prepare several ancillary programs which are used in Tesserae research and available through the web interface.  These are the benchmark Lucan-Vergil testing system, multi-text searching, and the old Tesserae versions 1 and 2.

If you're running the web interface and don't want to bother with these programs, you'll have to edit the PHP documents in the I<html/> directory to remove links to them.

=head2 Lucan-Vergil benchmark 

=head3 Description

For one particular comparison, Lucan's I<Pharsalia> Book 1 vs. Vergil's I<Aeneid>, we have a standard against which Tesserae results can be measured.  A set of 3,300 textual parallels was hand-graded by graduate students for their literary significance on a scale of 1 (worst) to 5 (best).  About 300 of these were cross-referenced against professional commentaries on Lucan, which represent a slightly different standard.

=head3 Setup

In order to enable tesserae to cross-check new results against the benchmark set, the human-entered data has to be correllated with the Tesserae database.  People entered textual parallels into a big CSV file using the textual I<locus>, e.g. "Book.Line", and usually they also entered a bit of the text of each phrase.  These need to be indexed against Tesserae's serially-numbered phrases for each text.  

Because the text may have changed and/or people may have entered data incorrectly, each locus is checked against all phrases known to Tesserae to begin at that locus; each of this is compared word by word to any text entered into the CSV, and the Tesserae phrase that matches the most text is selected.  In cases where the portion of text matched is very low, the Tesserae sentence and the human-entered text are both spit out for manual inspection.

In practice, the CSV file has been stable for a couple of months now, and you can trust that everything checks out.  All you have to do is run the following, and for now just ignore the debugging info it spits out.

B<% perl perl/benchmark/build-rec.pl>

=head3 Usage

To test a Tesserae search against the benchmark set, please see the pod documentation in cgi-bin/check-recall.pl

=head2 Multi-text searching

=head3 Description

This allows you to check the results of a Tesserae search against the rest of the corpus, to find out whether each parallel is unique or has precedents in other texts.  This is still under development, but is pretty interesting to the philologists.

=head3 Setup

It's necessary to index the entire corpus before multi-text searching will work.  Essentially, this involves indexing every line and every sentence in each text by every possible pair of words in it.  Run the following script.  It's a bit slow--it could really benefit from parallel processing.

C<% perl perl/v3/index_multi.pl>

=head3 Usage

Multi-text searching is applied as a second step to the binary file produced by a regular search, using I<cgi-bin/multitext.pl>.  This produces a new binary file.  To extract results in a usable format, use I<cgi-bin/read_multi.pl>.

Further details in the pod documentation of the two above scripts; see especially I<cgi-bin/read_multi.pl>.

=head2 Tesserae Version 1

=head3 Description

Version 1 worked significantly differently from Version 3 (the current version).  It searches on six-word windows rather than lines or phrases, and, at least at one time, it could return many-to-many parallels rather than just one-to-one.  A few people may still be using it; we're not sure.

=head3 Setup

To enable Version 1 searching, you have to create the v1 indices.  For now it only works on complete Latin texts, the ones in I<texts/la/>. Run the following two scripts to build the necessary word and stem indices, respectively:

B<% perl perl/v1/v1.prepare-texts.pl texts/la/*.tess>

B<% perl perl/v1/v1.stems.pl texts/la/*.tess>

=head3 Usage

Comparison is done using I<cgi-bin/session.pl>.  As far as I know, this is only ever run from the web interface any more.

=head2 Tesserae Version 2

=head3 Description

This is the precursor to the present version.  It searches on phrases, matching based on stems.  It also has a scoring system.

While this search does almost everything Version 3 does, it was very slow, particularly to set up, and I don't think very many people use it any longer.  It also at one time had a peculiar bug which caused some of the matching parallels to be dropped; this is reported to have been fixed.  On the official web site we don't ever update this part of the code, nor its stored binary files, because we can't figure out exactly how to replicate the last results we published, which were from before the bug fix.

Honestly, no one who works on the project today really knows how this works.  We should consider investing time in thoroughly documenting it or else we should eventually get rid of it.

=head3 Setup

Version 2 is set up using a two-stage process.  First, each text must be parsed using I<perl/v2/prepare.pl>. This creates a Storable binary in the directory I<data/v2/parsed/>.

B<% perl perl/v2/prepare.pl I<text1>>

where I<text1> is the complete filename of the text to be parsed, for example, I<texts/la/catullus.carmina.tess>.

Second, each I<pair> of texts you want to compare must be pre-processed using I<perl/v2/preprocess.pl>.  This step creates its own binary in the directory I<data/v2/preprocessed/>.

B<% perl perl/v2/preprocess.pl I<source> I<target>>

where I<source> and I<target> are the names of texts without the path or I<.tess> extension, for example, in the case of the file mentioned above, just I<catullus.carmina>.

If you want each of N texts to be comparable against each other one, you need to do this preprocessing step N*(N-1) times.  The last time anyone did a complete fresh install of V2 it was said to have taken 2 days to complete.  

=head3 Usage

You do a V2 search using I<cgi-bin/compare_texts.pl>.  Once again, this is nearly always run from the web interface these days.

There ought to be documentation available in pod format in that script, but I think it's pretty sparse, and to be honest, completing it is a low priority considering how infrequently the program is used.

=cut